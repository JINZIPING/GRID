# Semantic ID Standalone Module

A self-contained implementation of semantic ID generation using K-means clustering and residual quantization, without external dependencies beyond PyTorch and NumPy.

## ğŸ“ Project Structure

```
semantic_id_standalone_module/
â”œâ”€â”€ __init__.py                 # Main module exports
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ src/                        # Core implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ semantic_id_module.py   # Main implementation
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â”œâ”€â”€ train_example.py        # Training example
â”‚   â””â”€â”€ inference_example.py    # Inference example
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_complete.py        # Complete test suite
â””â”€â”€ docs/                       # Documentation
    â”œâ”€â”€ README.md               # Detailed documentation
    â””â”€â”€ SUMMARY.md              # Feature summary
```

## ğŸš€ Quick Start

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or install manually
pip install torch numpy
```

### Basic Usage

```python
from semantic_id_standalone_module import (
    ResidualQuantization,
    MiniBatchKMeans,
    SquaredEuclideanDistance,
    KMeansPlusPlusInitInitializer
)

# Create model
quantization_layer = MiniBatchKMeans(
    n_clusters=256,
    n_features=2048,
    distance_function=SquaredEuclideanDistance(),
    initializer=KMeansPlusPlusInitInitializer(
        n_clusters=256,
        distance_function=SquaredEuclideanDistance(),
        initialize_on_cpu=False
    ),
    init_buffer_size=3072,
    optimizer=None,
)

model = ResidualQuantization(
    n_layers=3,
    quantization_layer=quantization_layer,
    init_buffer_size=3072,
    normalize_residuals=True,
    train_layer_wise=True,
    track_residuals=True,
    verbose=True,
)

# Training
model.train()
# ... training loop ...

# Inference
model.eval()
with torch.no_grad():
    predictions = model.predict_step(embeddings)
```

## ğŸ“– Examples

- **Training**: See `examples/train_example.py`
- **Inference**: See `examples/inference_example.py`
- **Complete Test**: See `tests/test_complete.py`

## ğŸ”§ Configuration

The module uses the same parameters as the original implementation:

- `embedding_dim`: 2048 (model dimension)
- `num_hierarchies`: 3 (number of quantization layers)
- `codebook_width`: 256 (centroids per layer)
- `init_buffer_size`: 3072 (initialization buffer size)
- `optimizer`: SGD with lr=0.5

## ğŸ“š Documentation

- [Detailed Documentation](docs/README.md)
- [Feature Summary](docs/SUMMARY.md)

## âœ… Features

- âœ… Pure PyTorch implementation (no Lightning dependency)
- âœ… K-means++ initialization
- âœ… Residual quantization
- âœ… Mini-batch K-means clustering
- âœ… Configurable parameters
- âœ… GPU/CPU support
- âœ… Complete test suite
- âœ… Usage examples

## ğŸ”„ Migration from Original

This module is a standalone version of the original Lightning-based implementation. Key differences:

- Removed Lightning dependencies
- Simplified device management
- Self-contained implementation
- Same algorithms and parameters

## ğŸ“ License

Same as the original GRID project.
